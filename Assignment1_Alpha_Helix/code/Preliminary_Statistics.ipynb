{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract training split using train-test-split.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "base = os.path.dirname(os.path.dirname(os.path.abspath(\"Preliminary_Statistics.ipynb\")))\n",
    "split_file_name = base + \"\\data\\ArgumentAnnotatedEssays-2.0\\\\train-test-split.csv\"\n",
    "unified_data_file = base + '/data/unified_data_file.json'\n",
    "\n",
    "\n",
    "# convert csv file into DataFrame\n",
    "train_test_df = pd.read_csv(split_file_name, ';')\n",
    "\n",
    "# convert json into DataFrame\n",
    "unified_data_df = pd.read_json(unified_data_file)\n",
    "\n",
    "# add column \"SET\" from train_test_df to unified_data_df\n",
    "unified_data_df.insert(7,\"SET\",train_test_df.SET)\n",
    "\n",
    "\n",
    "# get training split from unified_data_df\n",
    "train_df = unified_data_df[unified_data_df.SET==\"TRAIN\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Number of essays, paragraphs, sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of essays = 322\n",
      "Number of paragraphs = 1464\n",
      "Number of sentences = 5237\n",
      "Number of tokens = 116501\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Computer number of essays\n",
    "num_of_essays = train_df.shape[0]\n",
    "print(f\"Number of essays = {num_of_essays}\")\n",
    "\n",
    "# Computer number of paragraphs\n",
    "num_pg_each_essay = train_df.text.apply(lambda x: len(x.split('\\n'))-2)\n",
    "num_of_pg = num_pg_each_essay.sum()\n",
    "print(f\"Number of paragraphs = {num_of_pg}\")\n",
    "\n",
    "\n",
    "# Computer number of sentences\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def num_of_sents(text: str) -> int:\n",
    "    parsed_essay = nlp(text.replace(\"\\n\",\" \"))    \n",
    "    sents = []\n",
    "    for sentence in parsed_essay.sents:\n",
    "        sents.append(sentence.text)\n",
    "    first_sen = sents[0]\n",
    "    sents.remove(first_sen)\n",
    "    return len(sents)\n",
    "\n",
    "num_sents_each_essay = train_df.text.apply(num_of_sents)\n",
    "num_sents = num_sents_each_essay.sum()\n",
    "print(f\"Number of sentences = {num_sents}\")\n",
    "\n",
    "# Computer number of tokens\n",
    "def num_of_tokens(text: str) -> int:\n",
    "    \n",
    "    #remove topic, just remain content of each essay\n",
    "    text_list = text.split(\"\\n\")\n",
    "    topic = text_list[0]\n",
    "    text_list.remove(topic)\n",
    "    content = \"\\n\".join(text_list)\n",
    "    \n",
    "    parsed_essay = nlp(content.strip())\n",
    "    tokens = []\n",
    "    for token in parsed_essay:\n",
    "        tokens.append(token.text)\n",
    "    return len(tokens)\n",
    "\n",
    "num_tokens_each_essay = train_df.text.apply(num_of_tokens)\n",
    "num_tokens = num_tokens_each_essay.sum()\n",
    "print(f\"Number of tokens = {num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Number of major claims, claims, premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Major Claims = 598\n",
      "Number of Claims = 1202\n",
      "Number of Premises = 3023\n"
     ]
    }
   ],
   "source": [
    "# Computer number of major claims\n",
    "mc_in_each_essay = train_df.major_claim.apply(lambda x: len(x))\n",
    "num_of_major_claims = mc_in_each_essay.sum()\n",
    "print(f\"Number of Major Claims = {num_of_major_claims}\")\n",
    "\n",
    "# Computer number of claims\n",
    "c_in_each_essay = train_df.claims.apply(lambda x: len(x))\n",
    "num_of_claims = c_in_each_essay.sum()\n",
    "print(f\"Number of Claims = {num_of_claims}\")\n",
    "\n",
    "# Computer number of premises\n",
    "pre_in_each_essay = train_df.premises.apply(lambda x: len(x))\n",
    "num_of_premises = pre_in_each_essay.sum()\n",
    "print(f\"Number of Premises = {num_of_premises}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Number of essays with and without confirmation bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of essays with confirmation bias = 122\n",
      "Number of essays without confirmation bias = 200\n"
     ]
    }
   ],
   "source": [
    "# Compute number of essays with confirmation bias\n",
    "with_bias = train_df.confirmation_bias.apply(lambda x: 1 if(x) else 0)\n",
    "num_of_essays_with_bias = with_bias.sum()\n",
    "print(f\"Number of essays with confirmation bias = {num_of_essays_with_bias}\") \n",
    "\n",
    "# Compute number of essays with confirmation bias\n",
    "without_bias = train_df.confirmation_bias.apply(lambda x: 0 if(x) else 1)\n",
    "num_of_essays_without_bias = without_bias.sum()\n",
    "print(f\"Number of essays without confirmation bias = {num_of_essays_without_bias}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Number of sufficient and insufficient paragraphs (arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of sufficient paragraphs = 538\n",
      "Numer of insufficient paragraphs = 282\n"
     ]
    }
   ],
   "source": [
    "# Computer number of sufficient paragraphs\n",
    "def count_suffi_pg(pgs: list) -> int:\n",
    "    num = 0\n",
    "    for pg in pgs:\n",
    "        if (pg[\"sufficient\"]):\n",
    "            num = num +1\n",
    "    return num\n",
    "    \n",
    "    \n",
    "suffi_pg_each_essay = train_df.paragraphs.apply(count_suffi_pg)\n",
    "num_suffi_pgs = suffi_pg_each_essay.sum()\n",
    "print(f\"Numer of sufficient paragraphs = {num_suffi_pgs}\")\n",
    "\n",
    "# Computer number of insufficient paragraphs\n",
    "def count_insuffi_pg(pgs: list) -> int:\n",
    "    num = 0\n",
    "    for pg in pgs:\n",
    "        if (not pg[\"sufficient\"]):\n",
    "            num = num +1\n",
    "    return num\n",
    "    \n",
    "    \n",
    "insuffi_pg_each_essay = train_df.paragraphs.apply(count_insuffi_pg)\n",
    "num_insuffi_pgs = insuffi_pg_each_essay.sum()\n",
    "print(f\"Numer of insufficient paragraphs = {num_insuffi_pgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Average number of tokens in major claims, claims and premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens in major claims = 14.695652173913043\n",
      "Average number of tokens in claims = 15.089850249584027\n",
      "Average number of tokens in premises = 17.601389348329473\n"
     ]
    }
   ],
   "source": [
    "# Computer number of tokens of a text in major_claims, claims, and premises\n",
    "def num_text_tokens(alist: list) -> int:\n",
    "    num = 0\n",
    "    for element in alist:\n",
    "        parsed_element = nlp(element[\"text\"])\n",
    "        tokens = []\n",
    "        for token in parsed_element:\n",
    "            tokens.append(token.text)\n",
    "        num = num + len(tokens)\n",
    "    return num\n",
    "\n",
    "# Computer average number of tokens in major claims\n",
    "num_mc_each_essay = train_df.major_claim.apply(num_text_tokens)\n",
    "num_tokens_major_claims = num_mc_each_essay.sum()\n",
    "print(f\"Average number of tokens in major claims = {num_tokens_major_claims/num_of_major_claims}\")\n",
    "\n",
    "# Computer average number of tokens in claims\n",
    "num_claims_each_essay = train_df.claims.apply(num_text_tokens)\n",
    "num_tokens_claims = num_claims_each_essay.sum()\n",
    "print(f\"Average number of tokens in claims = {num_tokens_claims/num_of_claims}\")\n",
    "\n",
    "# Computer average number of tokens in premises\n",
    "num_premises_each_essay = train_df.premises.apply(num_text_tokens)\n",
    "num_tokens_premises = num_premises_each_essay.sum()\n",
    "print(f\"Average number of tokens in premises = {num_tokens_premises/num_of_premises}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The 10 most specific words in major claims, claims, and premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most specific words in major claims: \n",
      "['services', 'listening', 'method', 'rural', 'offer', 'plan', 'cultures', 'favor', 'adults', 'academic']\n",
      "\n",
      "10 most specific words in claims: \n",
      "['issue', 'females', 'technologies', 'examination', 'destroy', 'suitable', 'increases', 'popular', 'powerful', 'touch']\n",
      "\n",
      "10 most specific words in premises: \n",
      "['know', 'concerned', 'adverse', 'reduced', 'change', 'tv', 'properly', 'employ', 'true', 'supporting']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def count_words(text: str) -> set:\n",
    "    words_set = set([])\n",
    "    parsed_text = nlp(text)\n",
    "    for token in parsed_text:\n",
    "        if(not token.is_punct and not token.is_space and not token.is_stop):\n",
    "            words_set.add(token.text.lower())\n",
    "    return words_set\n",
    "\n",
    "# Compute the key-words in essay topics\n",
    "topics_words_set_list = []\n",
    "for text in train_df.text:\n",
    "    text_list = text.split(\"\\n\")\n",
    "    topic = text_list[0]\n",
    "    topics_words_set = count_words(topic)\n",
    "    topics_words_set_list.append(topics_words_set)\n",
    "\n",
    "specific_words_topics = set([])\n",
    "for words_set in topics_words_set_list:\n",
    "    specific_words_topics = specific_words_topics ^ words_set\n",
    "    \n",
    "\n",
    "# Compute the 10 most specific words in major claims\n",
    "major_claims_words_set_list = []\n",
    "for mc_list in train_df.major_claim:\n",
    "    for mc in mc_list:\n",
    "        mc_words_set = count_words(mc[\"text\"])\n",
    "        major_claims_words_set_list.append(mc_words_set)\n",
    "\n",
    "specific_words_mc = set([])\n",
    "for words_set in major_claims_words_set_list:\n",
    "    specific_words_mc = specific_words_mc ^ words_set\n",
    "\n",
    "ten_spec_words_mc =  []\n",
    "mc_candidates = list(specific_words_mc & specific_words_topics)\n",
    "\n",
    "for i in range(10):\n",
    "    candidate = random.choice(mc_candidates)\n",
    "    mc_candidates.remove(candidate)\n",
    "    ten_spec_words_mc.append(candidate)\n",
    "    \n",
    "print(\"10 most specific words in major claims: \")\n",
    "print(f\"{ten_spec_words_mc}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Compute the 10 most specific words in claims\n",
    "claims_words_set_list = []\n",
    "for claim_list in train_df.claims:\n",
    "    for claim in claim_list:\n",
    "        claims_words_set = count_words(claim[\"text\"])\n",
    "        claims_words_set_list.append(claims_words_set)\n",
    "\n",
    "\n",
    "specific_words_claims = set([])\n",
    "for words_set in claims_words_set_list:\n",
    "    specific_words_claims = specific_words_claims ^ words_set\n",
    "\n",
    "ten_spec_words_claim = []\n",
    "claim_candidates = list(specific_words_claims & specific_words_topics)\n",
    "for i in range(10):\n",
    "    candidate = random.choice(claim_candidates)\n",
    "    claim_candidates.remove(candidate)\n",
    "    ten_spec_words_claim.append(candidate)\n",
    "    \n",
    "print(\"10 most specific words in claims: \")\n",
    "print(f\"{ten_spec_words_claim}\")\n",
    "print()\n",
    "\n",
    "# Compute the 10 most specific words in premises\n",
    "premises_words_set_list = []\n",
    "for premises_list in train_df.premises:\n",
    "    for premise in premises_list:\n",
    "        premises_words_set = count_words(premise[\"text\"])\n",
    "        premises_words_set_list.append(premises_words_set)\n",
    "\n",
    "\n",
    "specific_words_premises = set([])\n",
    "for words_set in premises_words_set_list:\n",
    "    specific_words_premises = specific_words_premises ^ words_set\n",
    "\n",
    "ten_spec_words_premise = []\n",
    "premise_candidates = list(specific_words_premises & specific_words_topics)\n",
    "for i in range(10):\n",
    "    candidate = random.choice(premise_candidates)\n",
    "    premise_candidates.remove(candidate)\n",
    "    ten_spec_words_premise.append(candidate)\n",
    "    \n",
    "print(\"10 most specific words in premises: \")\n",
    "print(f\"{ten_spec_words_premise}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
